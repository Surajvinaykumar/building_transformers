{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CqAAzfy4xYgJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, embed_size, heads):\n",
        "    super(SelfAttention, self).__init__()\n",
        "    self.embed_size = embed_size\n",
        "    self.heads = heads\n",
        "    self.head_dim = embed_size // heads\n",
        "\n",
        "    assert (self.head_dim * heads == embed_size), \"Embed size needs to be div by heads\"\n",
        "    self.values = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
        "    self.keys = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
        "    self.queries = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
        "    self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
        "\n",
        "  def forward(self, values, keys, query, mask):\n",
        "    N = query.shape[0]\n",
        "    value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "    #split embeddings into self.heads pieces\n",
        "    values = values.reshape(N, value_len, self.head_dim)\n",
        "    keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "    queries = query.reshape(N, key_len, self.heads, self.head_dim)\n",
        "\n",
        "    energy = torch.eimsum(\"nqhd, nkhd --> nhqk\", [queries, keys])# q= query length, h= head, d = heads dimension, n = batch size, k = key length, h = head, d= head dimension\n",
        "    #addiding a mask\n",
        "    if mask is not None:\n",
        "      energy = energy.masked_fill(mask == 0, float(-1e28))#if the leement of mask is 0 the shut it off\n",
        "    #pass this through softmax\n",
        "    attention = torch.softmax(energy / (self.embed_size ++ (1/2)), dim = 1)\n",
        "    out = torch.einsum(\"nhql, nlhd --> nqhd\", [attention, values]).reshape(N, query_len, self.heads*self.head_dim)\n",
        "    out = self.fc_out(out)\n",
        "    return out"
      ]
    }
  ]
}